{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for ABM timeseries scaling.\n",
    "\n",
    "This notebook takes the raw EU heating/cooling demand profiles from\n",
    "[Mopo AmBIENCe2ABM Demo](https://zenodo.org/records/10518294) `ideal_demands_XXXX.csv`s\n",
    "and `process_cops_XXXX.csv`s and experiments on them to see how they should be handled.\n",
    "\n",
    "The outline of this notebook is as follows:\n",
    "1. Julia environment setup to install the necessary dependencies.\n",
    "2. 1st approach at normalising the demand data, picking one year for normalisation doesn't work.\n",
    "3. 2nd approach at normalising the demand data using Eurostat HDD and CDD data, insufficient HDD data for the full scope.\n",
    "\n",
    "Should the COP timeseries be scaled?\n",
    "\n",
    "\n",
    "## Julia environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.instantiate()\n",
    "\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Dates\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read, inspect, and reorganize the raw data. (ATTEMPT 1)\n",
    "\n",
    "First, we'll need to define paths to the files we want to read.\n",
    "If the setup is done according to the instructions in `README.md`,\n",
    "then the following paths should work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Config for reading and normalising data.\n",
    "\n",
    "norm_year = 2012 # Year using which the results are normalised.\n",
    "years = [1995, 2008, 2009, 2012, 2015]\n",
    "dem_paths = [\n",
    "    year => \"input-data/abm-raw-data/ideal_demands_$(year).csv\"\n",
    "    for year in years\n",
    "]\n",
    "cop_paths = [\n",
    "    year => \"input-data/abm-raw-data/process_cops_$(year).csv\"\n",
    "    for year in years\n",
    "]\n",
    "hm_path = \"input-data/scen_current_building_demand/data/scen_current_building_demand.csv\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read and organise demands data.\n",
    "\n",
    "cols = [:timestamp, :year, :country, :category, :demand, :value]\n",
    "dem_data = DataFrame()\n",
    "for (year, path) in dem_paths\n",
    "    df = stack(DataFrame(CSV.File(path; header=[1,2])))\n",
    "    s = split.(df[!, :variable], '_')\n",
    "    df[!,:country] = string.(getindex.(s,1))\n",
    "    df[!,:category] = string.(getindex.(s,2))\n",
    "    df[!,:demand] = string.(getindex.(s,6))\n",
    "    df[!,:year] .= year\n",
    "    rename!(\n",
    "        df,\n",
    "        :building_archetype_building_process => :timestamp,\n",
    "    )\n",
    "    append!(dem_data, df[!, cols])\n",
    "end\n",
    "dem_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate sums over timestamp for the normalisation year.\n",
    "\n",
    "dem_sums = combine(\n",
    "    groupby(\n",
    "        filter(r -> r.year == norm_year, dem_data),\n",
    "        cols[2:end-1] # Skip timestamp and value.\n",
    "    ),\n",
    "    :value => sum\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalise dem_data using the normalisation year.\n",
    "\n",
    "dem_normalised = deepcopy(dem_data)\n",
    "for (i, df) in enumerate(groupby(dem_normalised, cols[3:end-1])) # skip timestamp, year, and value\n",
    "    df.value ./= dem_sums.value_sum[i]\n",
    "end\n",
    "dem_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect the results of the normalisation\n",
    "\n",
    "dem_normalised_sums = combine(\n",
    "    groupby(dem_normalised, cols[2:end-1]), # skip timestamp and value\n",
    "    :value => sum\n",
    ")\n",
    "dem_normalised_sums = unstack(dem_normalised_sums, :demand, :value_sum)\n",
    "describe(dem_normalised_sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Well that's bad.**\n",
    "\n",
    "DHW and heating behave more or less as I'd have expected,\n",
    "but cooling demand seems to vary quite a bit depending on the year.\n",
    "This seems to be the case in reality as well, as based on Eurostat HDD\n",
    "and CDD calculations, CDDs can vary between ~0-33x of the mean.\n",
    "Essentially, it becomes extremely important that our normalisation coefficients\n",
    "match the underlying scenario data as well as possible.\n",
    "\n",
    "\n",
    "### Solution?\n",
    "\n",
    "Perhaps we need to calculate the yearly normalisation coefficients based on historical heating and cooling degree day averages, as seems to have been done in Hotmaps?\n",
    "\n",
    ">the HDD and CDD on the NUTS3 level are calculated based on the average HDD (18.5/18.5) and CDD (22.5/22.5) calculated from the observed daily temperatures on a 25 x 25 km grid for the period 2002-2012 (see (Haylock, M.R. et al., 2011)).\n",
    "\n",
    "This should yield us ratios we can use to scale the profiles based on the climate year?\n",
    "\n",
    "\n",
    "## Read, inspect, and reorganize the raw data. (ATTEMPT 2)\n",
    "\n",
    "So it seems we need to be more clever with normalising our demand time series if we want them to vary reasonably with yearly weather.\n",
    "We'll need to use historical HDD and CDD from Eurostat for the normalisation instead it would seem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New settings for normalising based on HDD.\n",
    "\n",
    "norm_years = 2002:2012 # Hotmaps D5.2 year range for HDD and CDD calculations.\n",
    "hdd_path = \"input-data/eurostat/estat_nrg_chdd_a.tsv\"; # Heating and cooling degree days for scaling weather years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read and reformat HDD and CDD data\n",
    "\n",
    "hdd_data = DataFrame(CSV.File(hdd_path))\n",
    "s = split.(hdd_data[!,1], ',')\n",
    "hdd_data[!, :country] = string.(getindex.(s, 4))\n",
    "hdd_data[!, :variable] = string.(getindex.(s, 3))\n",
    "cols = [:country, :variable, :year, :value]\n",
    "hdd_data = stack(hdd_data; variable_name=:year)[!, cols]\n",
    "hdd_data[!, :year] = parse.(Int64, hdd_data[!, :year])\n",
    "filter!(r -> r.year in norm_years, hdd_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate 2002-2012 averages per country\n",
    "\n",
    "hdd_means = combine(\n",
    "    groupby(\n",
    "        hdd_data,\n",
    "        cols[1:2] # Group by country and variable\n",
    "    ),\n",
    "    :value => mean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate HDD and CDD scaling factors per country per year.\n",
    "\n",
    "hdd_scaling = deepcopy(hdd_data)\n",
    "for (i, df) in enumerate(groupby(hdd_scaling, cols[1:2]))\n",
    "    if hdd_means.value_mean[i] â‰ˆ 0 # This is required to avoid issues with Irish CDDs.\n",
    "        df.value .= 1.0\n",
    "    else\n",
    "        df.value ./= hdd_means.value_mean[i]\n",
    "    end\n",
    "end\n",
    "hdd_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect the scaling factors\n",
    "\n",
    "describe(\n",
    "    unstack(\n",
    "        hdd_scaling,\n",
    "        :variable,\n",
    "        :value\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is more manageable.**\n",
    "\n",
    "Heating demand seems to vary between ~ -35% and +37% percent from the 2002-2012 averages,\n",
    "while cooling demand still varies considerably between -100% and +7500%\n",
    "Ireland still causes some problems, as it doesn't have ANY cooling degree days during this period.\n",
    "\n",
    "Let's look at things on country-level next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect country HDD scaling ranges\n",
    "\n",
    "describe(\n",
    "    filter(\n",
    "        r -> r.variable == \"HDD\",\n",
    "        unstack(\n",
    "            hdd_scaling,\n",
    "            :country,\n",
    "            :value\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect country CDD scaling ranges\n",
    "\n",
    "describe(\n",
    "    filter(\n",
    "        r -> r.variable == \"CDD\",\n",
    "        unstack(\n",
    "            hdd_scaling,\n",
    "            :country,\n",
    "            :value\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In order to preserve weather variability and Hotmaps compatibility,\n",
    "the heating and cooling demands need to be scaled based on something like average HDDs and CDDs\n",
    "instead of any particular weather year.\n",
    "\n",
    "Unfortunately, the HDD and CDD data by Eurostat doesn't cover all the desired countries,\n",
    "and thus cannot be used for scaling the demands for Mopo WP5.\n",
    "Instead, I guess the second-best thing to do is just to normalise the demand time series using their overall mean values across the available data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setdiff(Set(dem_data.country), Set(hdd_data.country))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief inspection of COP data.\n",
    "\n",
    "Let's also quickly check the process COP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read and organize COP data\n",
    "\n",
    "cols = [:timestamp, :year, :country, :category, :process, :value]\n",
    "cop_data = DataFrame()\n",
    "for (year, path) in cop_paths\n",
    "    df = stack(DataFrame(CSV.File(path; header=[1,2])))\n",
    "    s = split.(df[!, :variable], '_')\n",
    "    df[!, :country] = string.(getindex.(s, 1))\n",
    "    df[!, :category] = string.(getindex.(s, 2))\n",
    "    df[!, :process] = string.(getindex.(s, 5)) .* '_' .* string.(get.(s, 6, \"air\"))\n",
    "    df[!, :year] .= year\n",
    "    rename!(\n",
    "        df,\n",
    "        :building_archetype_building_process => :timestamp\n",
    "    )\n",
    "    append!(cop_data, df[!, cols])\n",
    "end\n",
    "cop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect process data\n",
    "\n",
    "describe(\n",
    "    unstack(cop_data, :process, :value)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Honestly, these ranges seem surprisingly reasonable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
